{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn0xXl8idlIavwU+D+wgMg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarveshgadkari/ML/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsTbBjMelOaX"
      },
      "outputs": [],
      "source": [
        "#Linear\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# --------------------------\n",
        "# STEP 1: Load Data\n",
        "# --------------------------\n",
        "file_path = input(\"Enter the path to your CSV file: \")\n",
        "df = pd.read_csv(file_path)\n",
        "print(\"\\nâœ… Dataset Loaded Successfully!\")\n",
        "\n",
        "# --------------------------\n",
        "# STEP 2: Basic EDA\n",
        "# --------------------------\n",
        "print(\"\\n--- Dataset Info ---\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n--- First 5 Rows ---\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n--- Summary Statistics ---\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\n--- Missing Values ---\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n--- Data Types ---\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# --------------------------\n",
        "# STEP 3: Visual EDA\n",
        "# --------------------------\n",
        "print(\"\\nðŸ“Š Generating EDA plots...\")\n",
        "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Histograms\n",
        "df[numerical_cols].hist(figsize=(15, 10), bins=20)\n",
        "plt.suptitle(\"Histograms of Numerical Features\")\n",
        "plt.show()\n",
        "\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df[numerical_cols].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# --------------------------\n",
        "# STEP 4: Preprocessing\n",
        "# --------------------------\n",
        "# Encode categorical variables\n",
        "cat_cols = df.select_dtypes(include='object').columns\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    if df[col].nunique() == 2:\n",
        "        df[col] = le.fit_transform(df[col])  # Binary encoding\n",
        "    else:\n",
        "        df = pd.get_dummies(df, columns=[col], drop_first=True)  # One-hot encoding\n",
        "\n",
        "# Drop rows with missing values (optional: imputation)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# --------------------------\n",
        "# STEP 5: Feature Selection\n",
        "# --------------------------\n",
        "target_col = input(\"\\nEnter the name of the target column: \")\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "# Optional: Scaling\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# --------------------------\n",
        "# STEP 6: Train-Test Split\n",
        "# --------------------------\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --------------------------\n",
        "# STEP 7: Train Model\n",
        "# --------------------------\n",
        "model = LinearRegression()\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# --------------------------\n",
        "# STEP 8: Predict & Evaluate\n",
        "# --------------------------\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"\\nðŸ“ˆ Model Evaluation:\")\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared Score:\", r2)\n",
        "\n",
        "# --------------------------\n",
        "# STEP 9: Results Visualization\n",
        "# --------------------------\n",
        "result = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "print(\"\\n--- Prediction Results ---\")\n",
        "print(result.head())\n",
        "\n",
        "# Scatter Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.6, color='teal')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title('Actual vs Predicted Values')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DecisionTree\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --------------------------\n",
        "# STEP 1: Load Data\n",
        "# --------------------------\n",
        "file_path = input(\"Enter the path to your CSV file: \")\n",
        "df = pd.read_csv(file_path)\n",
        "print(\"\\nâœ… Dataset Loaded Successfully!\")\n",
        "\n",
        "# --------------------------\n",
        "# STEP 2: Basic EDA\n",
        "# --------------------------\n",
        "print(\"\\n--- Dataset Info ---\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\n--- First 5 Rows ---\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n--- Summary Statistics ---\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\n--- Missing Values ---\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n--- Data Types ---\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# --------------------------\n",
        "# STEP 3: Visual EDA\n",
        "# --------------------------\n",
        "print(\"\\nðŸ“Š Generating EDA plots...\")\n",
        "categorical_cols = df.select_dtypes(include='object').columns\n",
        "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Histograms for numerical features\n",
        "df[numerical_cols].hist(figsize=(15, 10), bins=20)\n",
        "plt.suptitle(\"Histograms of Numerical Features\")\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap for numerical features\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df[numerical_cols].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# --------------------------\n",
        "# STEP 4: Preprocessing\n",
        "# --------------------------\n",
        "# Encode categorical variables (handle binary and multi-class categories)\n",
        "df = pd.get_dummies(df, drop_first=True)  # One-hot encoding all categorical columns\n",
        "\n",
        "# Drop missing values (or handle with imputation)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# --------------------------\n",
        "# STEP 5: Feature Selection\n",
        "# --------------------------\n",
        "target_col = input(\"\\nEnter the name of the target column: \")\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "# --------------------------\n",
        "# STEP 6: Train-Test Split\n",
        "# --------------------------\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --------------------------\n",
        "# STEP 7: Train Decision Tree Models\n",
        "# --------------------------\n",
        "\n",
        "# 1. No Pruning\n",
        "model_1 = DecisionTreeClassifier(random_state=42)\n",
        "model_1.fit(x_train, y_train)\n",
        "y_pred_1 = model_1.predict(x_test)\n",
        "print(\"\\nAccuracy (No Pruning):\", accuracy_score(y_test, y_pred_1))\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(model_1, feature_names=X.columns, class_names=[str(i) for i in y.unique()], filled=True)\n",
        "plt.title(\"Decision Tree (No Pruning)\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Pre-Pruning\n",
        "model_2 = DecisionTreeClassifier(max_depth=5, min_samples_split=10, random_state=42)\n",
        "model_2.fit(x_train, y_train)\n",
        "y_pred_2 = model_2.predict(x_test)\n",
        "print(\"\\nAccuracy (Pre-Pruning):\", accuracy_score(y_test, y_pred_2))\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(model_2, feature_names=X.columns, class_names=[str(i) for i in y.unique()], filled=True)\n",
        "plt.title(\"Decision Tree (Pre-Pruning)\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Post-Pruning\n",
        "model_3 = DecisionTreeClassifier(random_state=42)\n",
        "model_3.fit(x_train, y_train)\n",
        "path = model_3.cost_complexity_pruning_path(x_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "models = [DecisionTreeClassifier(random_state=42, ccp_alpha=alpha).fit(x_train, y_train) for alpha in ccp_alphas]\n",
        "accuracies = [accuracy_score(y_test, model.predict(x_test)) for model in models]\n",
        "best_alpha = ccp_alphas[np.argmax(accuracies)]\n",
        "best_model = DecisionTreeClassifier(random_state=42, ccp_alpha=best_alpha)\n",
        "best_model.fit(x_train, y_train)\n",
        "y_pred_3 = best_model.predict(x_test)\n",
        "print(\"\\nAccuracy (Post-Pruning):\", accuracy_score(y_test, y_pred_3))\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(best_model, feature_names=X.columns, class_names=[str(i) for i in y.unique()], filled=True)\n",
        "plt.title(\"Decision Tree (Post-Pruning)\")\n",
        "plt.show()\n",
        "\n",
        "# --------------------------\n",
        "# STEP 8: Classification Report and Confusion Matrix\n",
        "# --------------------------\n",
        "\n",
        "# Classification Reports\n",
        "print(\"\\nClassification Report (No Pruning):\\n\", classification_report(y_test, y_pred_1))\n",
        "print(\"\\nClassification Report (Pre-Pruning):\\n\", classification_report(y_test, y_pred_2))\n",
        "print(\"\\nClassification Report (Post-Pruning):\\n\", classification_report(y_test, y_pred_3))\n",
        "\n",
        "# Confusion Matrices\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_1, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix (No Pruning)\")\n",
        "plt.show()\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_2, cmap='Purples')\n",
        "plt.title(\"Confusion Matrix (Pre-Pruning)\")\n",
        "plt.show()\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_3, cmap='Reds')\n",
        "plt.title(\"Confusion Matrix (Post-Pruning)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JacQrzPvllTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Naive Bayes\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('C:/Users/ADMIN/Desktop/ML/adult.csv')\n",
        "\n",
        "# Clean data: Handle '?' as NaN and drop missing rows\n",
        "df.replace(' ?', np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Encode categorical features\n",
        "le = LabelEncoder()\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == 'object':\n",
        "        df[column] = le.fit_transform(df[column])\n",
        "\n",
        "# Split features and target\n",
        "X = df.drop('income', axis=1)\n",
        "y = df['income']\n",
        "\n",
        "# Handle class imbalance (if needed)\n",
        "# Upsample minority class\n",
        "X_upsampled, y_upsampled = resample(X[y == 0], y[y == 0],\n",
        "                                    replace=True,\n",
        "                                    n_samples=X[y == 1].shape[0],\n",
        "                                    random_state=42)\n",
        "X_balanced = np.vstack((X[y == 1], X_upsampled))\n",
        "y_balanced = np.hstack((y[y == 1], y_upsampled))\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Naive Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_train = model.predict(X_train)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Accuracy Score (Test):\", accuracy_score(y_test, y_pred))\n",
        "print(\"Accuracy Score (Train):\", accuracy_score(y_train, y_pred_train))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "labels = ['<=50K', '>50K']\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Alternative Confusion Matrix Display\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Reds')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A60R12sUl-uY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Kmean\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Set Seaborn style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('C:/Users/ADMIN/Desktop/ML/Mall_Customers.csv')\n",
        "\n",
        "# Data overview\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\nMissing Values in each column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Visualize feature distributions\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=df, kde=True)\n",
        "plt.title(\"Feature Distributions\")\n",
        "plt.show()\n",
        "\n",
        "# Encode 'Genre' column (Male/Female)\n",
        "label_encoder = LabelEncoder()\n",
        "df['Genre'] = label_encoder.fit_transform(df['Genre'])\n",
        "\n",
        "# Select features for clustering\n",
        "X = df[['Annual Income (k$)', 'Spending Score (1-100)']]\n",
        "\n",
        "# Scatter plot of raw data\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], s=50)\n",
        "plt.title(\"Raw Data Points (Unlabeled)\")\n",
        "plt.xlabel(\"Annual Income (k$)\")\n",
        "plt.ylabel(\"Spending Score (1-100)\")\n",
        "plt.show()\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Elbow method to find optimal K\n",
        "inertia = []\n",
        "K_range = range(1, 11)\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(K_range, inertia, 'bo-')\n",
        "plt.title(\"Elbow Method For Optimal K\")\n",
        "plt.xlabel(\"Number of Clusters (K)\")\n",
        "plt.ylabel(\"Inertia (Within-Cluster Sum of Squares)\")\n",
        "plt.xticks(K_range)\n",
        "plt.show()\n",
        "\n",
        "# Optimal number of clusters selected based on elbow method\n",
        "optimal_k = 5  # because after k=5, there's no significant drop in inertia\n",
        "\n",
        "# Apply KMeans\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Visualize Clusters\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_kmeans, cmap='viridis', s=50)\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X', label='Centroids')\n",
        "plt.title(\"K-Means Clustering Result\")\n",
        "plt.xlabel(\"Annual Income (scaled)\")\n",
        "plt.ylabel(\"Spending Score (scaled)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Results\n",
        "print(\"Cluster Centers (in scaled space):\")\n",
        "print(centers)\n",
        "\n",
        "print(\"\\nFirst 10 Predicted Cluster Labels:\")\n",
        "print(y_kmeans[:10])\n"
      ],
      "metadata": {
        "id": "eloACh4cmiQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ANN\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Set Seaborn style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('C:/Users/ADMIN/Desktop/ML/Mall_Customers.csv')\n",
        "\n",
        "# Data overview\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\nMissing Values in each column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Visualize feature distributions\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=df, kde=True)\n",
        "plt.title(\"Feature Distributions\")\n",
        "plt.show()\n",
        "\n",
        "# Encode 'Genre' column (Male/Female)\n",
        "label_encoder = LabelEncoder()\n",
        "df['Genre'] = label_encoder.fit_transform(df['Genre'])\n",
        "\n",
        "# Select features for clustering\n",
        "X = df[['Annual Income (k$)', 'Spending Score (1-100)']]\n",
        "\n",
        "# Scatter plot of raw data\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], s=50)\n",
        "plt.title(\"Raw Data Points (Unlabeled)\")\n",
        "plt.xlabel(\"Annual Income (k$)\")\n",
        "plt.ylabel(\"Spending Score (1-100)\")\n",
        "plt.show()\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Elbow method to find optimal K\n",
        "inertia = []\n",
        "K_range = range(1, 11)\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(K_range, inertia, 'bo-')\n",
        "plt.title(\"Elbow Method For Optimal K\")\n",
        "plt.xlabel(\"Number of Clusters (K)\")\n",
        "plt.ylabel(\"Inertia (Within-Cluster Sum of Squares)\")\n",
        "plt.xticks(K_range)\n",
        "plt.show()\n",
        "\n",
        "# Optimal number of clusters selected based on elbow method\n",
        "optimal_k = 5  # because after k=5, there's no significant drop in inertia\n",
        "\n",
        "# Apply KMeans\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "y_kmeans = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Visualize Clusters\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_kmeans, cmap='viridis', s=50)\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X', label='Centroids')\n",
        "plt.title(\"K-Means Clustering Result\")\n",
        "plt.xlabel(\"Annual Income (scaled)\")\n",
        "plt.ylabel(\"Spending Score (scaled)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Results\n",
        "print(\"Cluster Centers (in scaled space):\")\n",
        "print(centers)\n",
        "\n",
        "print(\"\\nFirst 10 Predicted Cluster Labels:\")\n",
        "print(y_kmeans[:10])\n"
      ],
      "metadata": {
        "id": "lBkmq5IenCBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest\n",
        "# Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Function to Perform EDA (for any dataset)\n",
        "def perform_eda(df):\n",
        "    print(\"First few rows of the dataset:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nDataset Info:\")\n",
        "    print(df.info())\n",
        "    print(\"\\nSummary Statistics:\")\n",
        "    print(df.describe())\n",
        "    print(\"\\nMissing Values Check:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(\"\\nData Distribution Visualization:\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(df, kde=True)\n",
        "    plt.show()\n",
        "\n",
        "# Function to Preprocess Data (handle categorical and numerical data)\n",
        "def preprocess_data(df):\n",
        "    # Encode categorical columns using LabelEncoder\n",
        "    label_encoder = LabelEncoder()\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
        "    for col in categorical_columns:\n",
        "        df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df.drop(columns=['target'])  # Replace 'target' with your actual target column name\n",
        "    y = df['target']  # Replace 'target' with your actual target column name\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Feature scaling (optional, depending on model)\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Function to Perform Random Forest with Hyperparameter Tuning\n",
        "def random_forest_with_tuning(X_train, X_test, y_train, y_test):\n",
        "    # Hyperparameter grid for Random Forest\n",
        "    param_dist = {\n",
        "        'n_estimators': [50, 100, 150, 200],\n",
        "        'max_features': ['auto', 'sqrt', 'log2'],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'bootstrap': [True, False]\n",
        "    }\n",
        "\n",
        "    # Initialize RandomForestClassifier\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Randomized Search with Cross-Validation (StratifiedKFold)\n",
        "    random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
        "                                       n_iter=100, cv=StratifiedKFold(5),\n",
        "                                       verbose=2, random_state=42, n_jobs=-1)\n",
        "\n",
        "    # Fit the model\n",
        "    random_search.fit(X_train, y_train)\n",
        "\n",
        "    # Best hyperparameters\n",
        "    print(\"Best Hyperparameters Found: \", random_search.best_params_)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = random_search.best_estimator_.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"\\nAccuracy:\", accuracy)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Main function to execute\n",
        "def main():\n",
        "    # Load dataset (Replace with the path to your dataset)\n",
        "    df = pd.read_csv('C:/path/to/your/dataset.csv')\n",
        "\n",
        "    # Perform EDA\n",
        "    perform_eda(df)\n",
        "\n",
        "    # Preprocess data\n",
        "    X_train, X_test, y_train, y_test = preprocess_data(df)\n",
        "\n",
        "    # Perform Random Forest with Hyperparameter Tuning\n",
        "    random_forest_with_tuning(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "RKp-FXGNozPo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}